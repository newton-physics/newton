{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robot Policy Control\n",
    "\n",
    "[![ Click here to deploy.](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://brev.nvidia.com/launchable/deploy?launchableID=env-35QaaoiXx6VDmVNBOEtmpLs9VBs)\n",
    "\n",
    "This notebook demonstrates how to load and run a trained reinforcement learning policy on the Unitree Go2 quadruped robot. The policy was trained in IsaacLab using PhysX simulation.\n",
    "\n",
    "Key topics:\n",
    "- Loading pre-trained PyTorch policies\n",
    "- Computing observations for policy input\n",
    "- Understanding reward functions\n",
    "- Running closed-loop control with MuJoCo solver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newton\n",
    "import warp as wp\n",
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Robot Assets\n",
    "\n",
    "Newton provides utilities to download robot models and trained policies from the `newton-assets` repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Go2 robot assets (USD model, policy, config)\n",
    "asset_directory = str(newton.utils.download_asset(\"unitree_go2\"))\n",
    "\n",
    "# Load configuration from YAML\n",
    "config_path = f\"{asset_directory}/rl_policies/go2.yaml\"\n",
    "with open(config_path, encoding=\"utf-8\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Number of DOFs: {config['num_dofs']}\")\n",
    "print(f\"Action scale: {config['action_scale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "We'll load the Go2 robot from USD and configure it for simulation with the MuJoCo solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model builder\n",
    "builder = newton.ModelBuilder(up_axis=newton.Axis.Z)\n",
    "\n",
    "# Register MuJoCo-specific attributes\n",
    "newton.solvers.SolverMuJoCo.register_custom_attributes(builder)\n",
    "\n",
    "# Configure joint defaults\n",
    "builder.default_joint_cfg = newton.ModelBuilder.JointDofConfig(\n",
    "    armature=0.1,\n",
    "    limit_ke=1.0e2,\n",
    "    limit_kd=1.0e0,\n",
    ")\n",
    "\n",
    "# Configure contact parameters\n",
    "builder.default_shape_cfg.ke = 5.0e4  # Contact stiffness\n",
    "builder.default_shape_cfg.kd = 5.0e2  # Contact damping\n",
    "builder.default_shape_cfg.kf = 1.0e3  # Friction stiffness\n",
    "builder.default_shape_cfg.mu = 0.75   # Friction coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Go2 robot from USD\n",
    "builder.add_usd(\n",
    "    f\"{asset_directory}/usd/go2.usda\",\n",
    "    xform=wp.transform(wp.vec3(0, 0, 0.8)),\n",
    "    collapse_fixed_joints=False,\n",
    "    enable_self_collisions=False,\n",
    "    hide_collision_shapes=True,\n",
    ")\n",
    "\n",
    "# Approximate mesh colliders with convex hulls\n",
    "builder.approximate_meshes(\"convex_hull\")\n",
    "\n",
    "# Add ground plane\n",
    "builder.add_ground_plane()\n",
    "\n",
    "print(f\"Robot loaded: {builder.body_count} bodies, {builder.joint_count} joints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Initial State and Joint Properties\n",
    "\n",
    "Set the initial pose and configure joint stiffness/damping for PD control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial floating base pose (position + quaternion)\n",
    "builder.joint_q[:3] = [0.0, 0.0, 0.76]  # Position\n",
    "builder.joint_q[3:7] = [0.0, 0.0, 0.7071, 0.7071]  # Quaternion (facing forward)\n",
    "\n",
    "# Set initial joint positions from config\n",
    "builder.joint_q[7:] = config[\"mjw_joint_pos\"]\n",
    "\n",
    "# Configure joint PD gains and armature\n",
    "for i in range(config['num_dofs']):\n",
    "    builder.joint_target_ke[i + 6] = config[\"mjw_joint_stiffness\"][i]\n",
    "    builder.joint_target_kd[i + 6] = config[\"mjw_joint_damping\"][i]\n",
    "    builder.joint_armature[i + 6] = config[\"mjw_joint_armature\"][i]\n",
    "\n",
    "print(\"Initial state configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the model\n",
    "model = builder.finalize()\n",
    "model.set_gravity((0.0, 0.0, -9.81))\n",
    "\n",
    "print(f\"Model finalized: {model.body_count} bodies, {model.joint_count} joints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Solver and States\n",
    "\n",
    "We use the MuJoCo solver for accurate articulated body dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MuJoCo solver\n",
    "solver = newton.solvers.SolverMuJoCo(\n",
    "    model,\n",
    "    use_mujoco_cpu=False,\n",
    "    solver=\"newton\",\n",
    "    nconmax=30,\n",
    "    njmax=100,\n",
    ")\n",
    "\n",
    "# Create state objects\n",
    "state_0 = model.state()\n",
    "state_1 = model.state()\n",
    "control = model.control()\n",
    "\n",
    "# Evaluate forward kinematics\n",
    "newton.eval_fk(model, state_0.joint_q, state_0.joint_qd, state_0)\n",
    "\n",
    "print(\"Solver and states initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Policy\n",
    "\n",
    "The policy is a neural network trained with reinforcement learning in IsaacLab. It maps observations to joint position targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup PyTorch device\n",
    "device = wp.get_device()\n",
    "torch_device = \"cuda\" if device.is_cuda else \"cpu\"\n",
    "\n",
    "# Load policy\n",
    "policy_path = f\"{asset_directory}/rl_policies/physx_go2.pt\"\n",
    "policy = torch.jit.load(policy_path, map_location=torch_device)\n",
    "\n",
    "print(f\"Policy loaded from: {policy_path}\")\n",
    "print(f\"Device: {torch_device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and Reward Functions\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The policy receives observations consisting of:\n",
    "- Base linear velocity (body frame): 3D\n",
    "- Base angular velocity (body frame): 3D\n",
    "- Projected gravity vector: 3D\n",
    "- Command (forward, lateral, yaw): 3D\n",
    "- Joint positions (relative to default): 23D\n",
    "- Joint velocities: 23D\n",
    "- Previous actions: 23D\n",
    "\n",
    "Total: **81 dimensions**\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "The policy was trained to maximize rewards based on:\n",
    "\n",
    "**Tracking rewards:**\n",
    "- Linear velocity tracking: $r_{\\text{lin}} = \\exp(-||v_{xy} - v_{\\text{cmd}}||^2 / \\sigma)$\n",
    "- Angular velocity tracking: $r_{\\text{ang}} = \\exp(-|\\omega_z - \\omega_{\\text{cmd}}|^2 / \\sigma)$\n",
    "\n",
    "**Regularization penalties:**\n",
    "- Action rate: $-||a_t - a_{t-1}||^2$ (smooth actions)\n",
    "- Joint acceleration: $-||\\ddot{q}||^2$ (smooth motion)\n",
    "- Torque: $-||\\tau||^2$ (energy efficiency)\n",
    "- Base height: $-(z - z_{\\text{target}})^2$ (maintain upright posture)\n",
    "- Orientation: $-||q_{xy}||^2$ (stay upright)\n",
    "\n",
    "The total reward is a weighted sum of these terms, with weights tuned during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Computation\n",
    "\n",
    "Helper function to compute policy observations from simulation state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def quat_rotate_inverse(q: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Rotate vector by inverse of quaternion (world to body frame).\"\"\"\n",
    "    q_w = q[..., 3]\n",
    "    q_vec = q[..., :3]\n",
    "    a = v * (2.0 * q_w**2 - 1.0).unsqueeze(-1)\n",
    "    b = torch.cross(q_vec, v, dim=-1) * q_w.unsqueeze(-1) * 2.0\n",
    "    c = q_vec * torch.bmm(q_vec.view(q.shape[0], 1, 3), v.view(q.shape[0], 3, 1)).squeeze(-1) * 2.0\n",
    "    return a - b + c\n",
    "\n",
    "\n",
    "def compute_obs(\n",
    "    state: newton.State,\n",
    "    joint_pos_initial: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    command: torch.Tensor,\n",
    "    indices: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute observation vector for policy.\"\"\"\n",
    "    # Extract state (floating base + joints)\n",
    "    root_quat = torch.tensor(state.joint_q[3:7], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    root_lin_vel = torch.tensor(state.joint_qd[:3], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    root_ang_vel = torch.tensor(state.joint_qd[3:6], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    joint_pos = torch.tensor(state.joint_q[7:], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    joint_vel = torch.tensor(state.joint_qd[6:], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Transform to body frame\n",
    "    vel_b = quat_rotate_inverse(root_quat, root_lin_vel)\n",
    "    ang_vel_b = quat_rotate_inverse(root_quat, root_ang_vel)\n",
    "    gravity_vec = torch.tensor([0.0, 0.0, -1.0], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "    grav_b = quat_rotate_inverse(root_quat, gravity_vec)\n",
    "    \n",
    "    # Compute relative joint positions and velocities\n",
    "    joint_pos_rel = joint_pos - joint_pos_initial\n",
    "    \n",
    "    # Reorder joints to match policy training (PhysX ordering)\n",
    "    joint_pos_rel = torch.index_select(joint_pos_rel, 1, indices)\n",
    "    joint_vel = torch.index_select(joint_vel, 1, indices)\n",
    "    \n",
    "    # Concatenate observation\n",
    "    obs = torch.cat([vel_b, ang_vel_b, grav_b, command, joint_pos_rel, joint_vel, actions], dim=1)\n",
    "    \n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Policy Tensors\n",
    "\n",
    "Setup joint mappings and initial tensors for policy execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint ordering mapping (PhysX training vs MuJoCo Warp)\n",
    "def find_joint_mapping(mjw_names, physx_names):\n",
    "    \"\"\"Map between MuJoCo Warp and PhysX joint orderings.\"\"\"\n",
    "    mjw_to_physx = [physx_names.index(j) for j in mjw_names if j in physx_names]\n",
    "    physx_to_mjw = [mjw_names.index(j) for j in physx_names if j in mjw_names]\n",
    "    return mjw_to_physx, physx_to_mjw\n",
    "\n",
    "mjw_to_physx, physx_to_mjw = find_joint_mapping(\n",
    "    config[\"mjw_joint_names\"],\n",
    "    config[\"physx_joint_names\"]\n",
    ")\n",
    "\n",
    "# Convert to torch tensors\n",
    "physx_to_mjw_indices = torch.tensor(physx_to_mjw, device=torch_device, dtype=torch.long)\n",
    "mjw_to_physx_indices = torch.tensor(mjw_to_physx, device=torch_device, dtype=torch.long)\n",
    "\n",
    "# Initialize policy state\n",
    "joint_pos_initial = torch.tensor(state_0.joint_q[7:], device=torch_device, dtype=torch.float32).unsqueeze(0)\n",
    "actions = torch.zeros(1, config['num_dofs'], device=torch_device, dtype=torch.float32)\n",
    "command = torch.tensor([[1.0, 0.0, 1.0]], device=torch_device, dtype=torch.float32)  # Walk forward\n",
    "\n",
    "print(f\"Policy tensors initialized\")\n",
    "print(f\"Command: forward={command[0,0]:.1f}, lateral={command[0,1]:.1f}, yaw={command[0,2]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Loop\n",
    "\n",
    "Run closed-loop control: observe → policy → act → simulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "fps = 200\n",
    "frame_dt = 1.0 / fps\n",
    "decimation = 4  # Policy runs at 50 Hz (200 / 4)\n",
    "sim_substeps = 8  # Must be even for proper state swapping\n",
    "sim_dt = frame_dt / sim_substeps\n",
    "\n",
    "print(f\"Simulation: {fps} Hz, Policy: {fps/decimation} Hz\")\n",
    "print(f\"Substeps: {sim_substeps}, dt: {sim_dt:.6f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_step():\n",
    "    global state_0, state_1\n",
    "    \"\"\"Run multiple physics substeps.\"\"\"\n",
    "    for i in range(sim_substeps):\n",
    "        state_0.clear_forces()\n",
    "        contacts = model.collide(state_0)\n",
    "        solver.step(state_0, state_1, control, contacts, sim_dt)\n",
    "        state_0, state_1 = state_1, state_0\n",
    "\n",
    "\n",
    "def policy_step():\n",
    "    \"\"\"Compute policy action and update control.\"\"\"\n",
    "    global actions\n",
    "    \n",
    "    # Compute observation\n",
    "    obs = compute_obs(state_0, joint_pos_initial, actions, command, physx_to_mjw_indices)\n",
    "    \n",
    "    # Run policy\n",
    "    with torch.no_grad():\n",
    "        actions = policy(obs)\n",
    "    \n",
    "    # Reorder actions to MuJoCo Warp joint ordering\n",
    "    actions_mjw = torch.index_select(actions, 1, mjw_to_physx_indices)\n",
    "    \n",
    "    # Compute joint targets (default + scaled action)\n",
    "    joint_targets = joint_pos_initial + config[\"action_scale\"] * actions_mjw\n",
    "    \n",
    "    # Add zeros for floating base (6 DOF) and convert to Warp\n",
    "    targets_with_base = torch.cat([\n",
    "        torch.zeros(6, device=torch_device, dtype=torch.float32),\n",
    "        joint_targets.squeeze(0)\n",
    "    ])\n",
    "    \n",
    "    # Update control\n",
    "    control.joint_target_pos.assign(wp.from_torch(targets_with_base, dtype=wp.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration with CUDA Graphs\n",
    "\n",
    "For maximum performance, capture the simulation loop as a CUDA graph to reduce kernel launch overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture simulation as CUDA graph (if running on GPU)\n",
    "graph = None\n",
    "if wp.get_device().is_cuda and wp.is_mempool_enabled(wp.get_device()):\n",
    "    print(\"Capturing CUDA graph for optimized execution...\")\n",
    "    with wp.ScopedCapture() as capture:\n",
    "        simulate_step()\n",
    "    graph = capture.graph\n",
    "    print(\"CUDA graph captured successfully\")\n",
    "else:\n",
    "    print(\"Running on CPU (no CUDA graph)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation with Viewer\n",
    "\n",
    "Execute the policy and visualize the robot walking forward. The CUDA graph accelerates the physics simulation significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create viewer\n",
    "viewer = newton.viewer.ViewerRerun(keep_historical_data=True)\n",
    "viewer.set_model(model)\n",
    "\n",
    "# Simulation loop\n",
    "num_frames = 2500\n",
    "sim_time = 0.0\n",
    "\n",
    "for frame in trange(num_frames, desc=\"Simulating Go2 walking\"):\n",
    "    # Run policy every decimation steps\n",
    "    if frame % decimation == 0:\n",
    "        policy_step()\n",
    "    \n",
    "    # Physics step (use CUDA graph if available)\n",
    "    if graph:\n",
    "        wp.capture_launch(graph)\n",
    "    else:\n",
    "        simulate_step()\n",
    "    \n",
    "    # Log to viewer\n",
    "    viewer.begin_frame(sim_time)\n",
    "    viewer.log_state(state_0)\n",
    "    viewer.end_frame()\n",
    "    \n",
    "    sim_time += frame_dt\n",
    "\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded a pre-trained policy** from PyTorch JIT format\n",
    "2. **Built the Go2 robot model** from USD with proper contact parameters\n",
    "3. **Configured PD controllers** with joint stiffness and damping\n",
    "4. **Computed observations** including base velocities, gravity, and joint states\n",
    "5. **Understood reward functions** used during RL training\n",
    "6. **Ran closed-loop control** with the MuJoCo solver\n",
    "7. **Visualized results** in Rerun\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Policy Network**: Maps observations → actions (joint position offsets)\n",
    "- **PD Control**: Tracks policy targets with stiffness/damping gains\n",
    "- **Joint Ordering**: PhysX training order vs MuJoCo Warp simulation order\n",
    "- **Observation Space**: Body-frame velocities + proprioception + commands\n",
    "- **Reward Shaping**: Balance tracking performance with regularization\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Modify the command vector to control walking direction\n",
    "- Experiment with different robots (Go2, Anymal), see `example_robot_policy.py` in the Newton examples\n",
    "- Train your own policies using IsaacLab\n",
    "- Add terrain or obstacles to test robustness\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
